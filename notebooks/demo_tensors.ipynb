{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d5e993",
   "metadata": {},
   "source": [
    "# Demo Tensors Dataset (HDF5)\n",
    "\n",
    "This notebook demonstrates the generation and usage of the Demo Tensors dataset, which includes a diverse range of tensor dimensions (0D to 5D).\n",
    "\n",
    "The dataset is generated by `scripts/generate_tensors.py` and results in an HDF5 file (`.h5`) located at `data/synthetic/demo_tensors/demo_tensors.h5` (relative to the repository root).\n",
    "\n",
    "**Note**: To run this notebook, you will need the `h5py` library. If you haven't installed it yet, you can do so by running:\n",
    "```bash\n",
    "pip install h5py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-generate",
   "metadata": {},
   "source": [
    "## 1. Generate the Dataset\n",
    "\n",
    "The following cell executes the script `scripts/generate_tensors.py`.\n",
    "This script now generates several tensors and saves them into `demo_tensors.h5`: \n",
    "`tensor_a`, `tensor_b`, `scalar_data`, `vector_data`, `image_grayscale_data`, `image_rgb_data`, `video_frames_data`, and `simulation_data`.\n",
    "\n",
    "Since this notebook is in the `notebooks/` directory, we use `../` to correctly path to the script and the output directory.\n",
    "This command will create (or overwrite) `../data/synthetic/demo_tensors/demo_tensors.h5`. Use `--seed` for deterministic output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62464c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/generate_tensors.py ../data/synthetic/demo_tensors --seed 123"
  ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown-load",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect the Data from HDF5 File\n",
    "\n",
    "Now, we'll load the generated `demo_tensors.h5` file using the `h5py` library.\n",
    "We will then list all datasets (tensors) stored within the file and inspect each one, showing its shape, data type (dtype), and for the scalar, its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b990e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np # Still useful for verifying dtypes or further operations\n",
    "\n",
    "# Define the path to the data file (relative to this notebook)\n",
    "data_path = '../data/synthetic/demo_tensors/demo_tensors.h5'\n",
    "\n",
    "print(f\"Attempting to load data from: {data_path}\\n\")\n",
    "\n",
    "with h5py.File(data_path, 'r') as hf:\n",
    "    print(f\"Available datasets in HDF5: {sorted(list(hf.keys()))}\\n\")\n",
    "\n",
    "    # Define the tensors we expect to find (and their descriptions for printing)\n",
    "    tensor_info = {\n",
    "        \"tensor_a\": \"Original general 2D tensor\",\n",
    "        \"tensor_b\": \"Original general 3D tensor\",\n",
    "        \"scalar_data\": \"0D tensor (scalar)\",\n",
    "        \"vector_data\": \"1D tensor (vector)\",\n",
    "        \"image_grayscale_data\": \"2D tensor (grayscale image)\",\n",
    "        \"image_rgb_data\": \"3D tensor (RGB image)\",\n",
    "        \"video_frames_data\": \"4D tensor (sequence of grayscale frames)\",\n",
    "        \"simulation_data\": \"5D tensor (higher-dimensional data)\"\n",
    "    }\n",
    "\n",
    "    for name, description in tensor_info.items():\n",
    "        if name in hf:\n",
    "            dataset = hf[name] # Access the HDF5 dataset object\n",
    "            print(f\"Dataset: '{name}' ({description})\")\n",
    "            print(f\"  Shape: {dataset.shape}\")\n",
    "            print(f\"  dtype: {dataset.dtype}\")\n",
    "            \n",
    "            # Load data into memory as NumPy array to work with it\n",
    "            if name == 'scalar_data':\n",
    "                value = dataset[()] # Use [()] for scalars\n",
    "                print(f\"  Value: {value}\")\n",
    "            else:\n",
    "                # For other tensors, you might load them fully using [:]\n",
    "                # array_data = dataset[:] \n",
    "                # print(f\"  First element if loaded: {array_data.flatten()[0] if array_data.size > 0 else 'N/A'}\")\n",
    "                pass # Avoid loading large arrays fully in this overview\n",
    "            print(\"---\")\n",
    "        else:\n",
    "            print(f\"Dataset: '{name}' was not found in the file.\")\n",
    "            print(\"---\")\n",
    "\n",
    "# The file hf is automatically closed when exiting the 'with' block."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
